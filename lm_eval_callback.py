"""
LM-Eval Callback for Training

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¿è¡Œ lm-evaluation-harness è¯„ä¼°ï¼Œç›‘æ§æ¨¡å‹èƒ½åŠ›å˜åŒ–ã€‚
ç”¨äºæ£€æµ‹ç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰å’Œæ¨¡å¼å¡Œç¼©ã€‚

Features:
- è½»é‡çº§è¯„ä¼°ï¼šä½¿ç”¨ --limit é™åˆ¶æ ·æœ¬æ•°ï¼ˆå›ºå®šå–å‰ N æ¡ï¼Œä¿è¯å¯æ¯”æ€§ï¼‰
- å¤ç”¨ checkpointï¼šä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­å·²ä¿å­˜çš„ checkpointï¼Œæ— éœ€é¢å¤–ä¿å­˜
- åŒé‡è®°å½•ï¼šåŒæ—¶è®°å½•ç»å¯¹åˆ†æ•°å’Œç›¸å¯¹åŸºçº¿çš„ delta
- è‡ªåŠ¨ä¸Šä¼ ï¼šå°†è¯„ä¼°ç»“æœä¸Šä¼ åˆ° WandB
"""

import os
import json
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

import torch
from transformers import TrainerCallback, TrainerControl, TrainerState
from transformers.training_args import TrainingArguments


class LMEvalCallback(TrainerCallback):
    """
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¿è¡Œ lm-eval è¯„ä¼°çš„ Callback
    
    Args:
        eval_steps: æ¯éš”å¤šå°‘æ­¥è¿è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆé»˜è®¤ 100ï¼Œå¿…é¡»æ˜¯ save_steps çš„å€æ•°ï¼‰
        tasks: è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼ˆé»˜è®¤ ['mmlu_stem', 'mmlu_other', 'gsm8k']ï¼‰
        limit: æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬æ•°é™åˆ¶ï¼ˆé»˜è®¤ 100ï¼Œç”¨äºå¿«é€Ÿè¯„ä¼°ï¼‰
        num_fewshot: few-shot æ ·æœ¬æ•°ï¼ˆé»˜è®¤ 3ï¼‰
        batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 8ï¼‰
        output_dir: è¯„ä¼°ç»“æœä¿å­˜ç›®å½•
        log_to_wandb: æ˜¯å¦è®°å½•åˆ° WandB
    """
    
    def __init__(
        self,
        eval_steps: int = 100,  # Must be multiple of save_steps (100)
        tasks: List[str] = None,
        limit: int = 100,
        num_fewshot: int = 3,
        batch_size: int = 8,
        output_dir: str = None,
        log_to_wandb: bool = True,
    ):
        self.eval_steps = eval_steps
        self.tasks = tasks or ['mmlu_stem', 'mmlu_other', 'gsm8k']
        self.limit = limit
        self.num_fewshot = num_fewshot
        self.batch_size = batch_size
        self.output_dir = output_dir
        self.log_to_wandb = log_to_wandb
        
        # å­˜å‚¨åŸºçº¿åˆ†æ•°ç”¨äºå¯¹æ¯”
        self.baseline_scores: Dict[str, float] = {}
        self.eval_history: List[Dict[str, Any]] = []
        
        # è®°å½•ä¸Šæ¬¡è¯„ä¼°æ­¥æ•°ï¼Œé¿å…é‡å¤è¯„ä¼°
        self.last_eval_step = -1
        
        # çŠ¶æ€æ–‡ä»¶åï¼ˆç”¨äºæ–­ç‚¹æ¢å¤ï¼‰
        self.state_filename = "lm_eval_state.json"
    
    def _get_state_path(self, output_dir: str) -> str:
        """è·å–çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
        return os.path.join(output_dir, self.state_filename)
    
    def _save_state(self, output_dir: str):
        """ä¿å­˜ callback çŠ¶æ€åˆ°æ–‡ä»¶ï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰"""
        state = {
            "baseline_scores": self.baseline_scores,
            "eval_history": self.eval_history,
            "last_eval_step": self.last_eval_step,
            "tasks": self.tasks,
            "limit": self.limit,
        }
        
        state_path = self._get_state_path(output_dir)
        try:
            with open(state_path, 'w') as f:
                json.dump(state, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ Failed to save LM-Eval state: {e}")
    
    def _load_state(self, output_dir: str) -> bool:
        """ä»æ–‡ä»¶åŠ è½½ callback çŠ¶æ€ï¼ˆæ–­ç‚¹æ¢å¤æ—¶ä½¿ç”¨ï¼‰"""
        state_path = self._get_state_path(output_dir)
        
        if not os.path.exists(state_path):
            return False
        
        try:
            with open(state_path, 'r') as f:
                state = json.load(f)
            
            self.baseline_scores = state.get("baseline_scores", {})
            self.eval_history = state.get("eval_history", [])
            self.last_eval_step = state.get("last_eval_step", -1)
            
            print(f"âœ… Restored LM-Eval state from {state_path}")
            print(f"   Last eval step: {self.last_eval_step}")
            print(f"   Baseline scores: {self.baseline_scores}")
            print(f"   Eval history: {len(self.eval_history)} records")
            
            return True
        except Exception as e:
            print(f"âš ï¸ Failed to load LM-Eval state: {e}")
            return False
        
    def on_save(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """
        åœ¨ checkpoint ä¿å­˜ä¹‹åè¿è¡Œ LM-Eval è¯„ä¼°
        
        ä½¿ç”¨ on_save è€Œä¸æ˜¯ on_step_endï¼Œç¡®ä¿ checkpoint å·²ä¿å­˜åå†è¯„ä¼°
        """
        
        # åªåœ¨ä¸»è¿›ç¨‹ä¸Šè¿è¡Œè¯„ä¼°
        if args.local_rank not in [-1, 0]:
            return
        
        current_step = state.global_step
        
        # é¿å…é‡å¤è¯„ä¼°
        if current_step == self.last_eval_step:
            return
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ lm_eval è¯„ä¼°ç‚¹
        if current_step % self.eval_steps != 0:
            return
        
        self.last_eval_step = current_step
        
        print(f"\n{'='*60}")
        print(f"ğŸ” Running LM-Eval at step {current_step} (checkpoint just saved)")
        print(f"{'='*60}")
        
        try:
            # è¿è¡Œè¯„ä¼°
            results = self._run_evaluation(model, args, state)
            
            if results:
                # è®°å½•ç»“æœ
                self._log_results(results, current_step, args)
                
                # æ‰“å°æ‘˜è¦
                self._print_summary(results, current_step)
                
        except Exception as e:
            print(f"âš ï¸ LM-Eval evaluation failed: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"{'='*60}\n")
    
    def on_train_begin(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®åŸºçº¿åˆ†æ•°ï¼Œæˆ–ä»æ–­ç‚¹æ¢å¤çŠ¶æ€"""
        
        # åªåœ¨ä¸»è¿›ç¨‹ä¸Šè¿è¡Œ
        if args.local_rank not in [-1, 0]:
            return
        
        eval_output_dir = self.output_dir or args.output_dir
        
        # å°è¯•ä»æ–‡ä»¶æ¢å¤çŠ¶æ€ï¼ˆæ–­ç‚¹æ¢å¤ï¼‰
        if state.global_step > 0:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Resuming from step {state.global_step}, restoring LM-Eval state...")
            print(f"{'='*60}")
            
            if self._load_state(eval_output_dir):
                # æˆåŠŸæ¢å¤çŠ¶æ€
                print(f"{'='*60}\n")
                return
            else:
                # æ¢å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸºçº¿
                print(f"âš ï¸ No saved state found, using default baselines")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Setting baseline scores (from pre-evaluated Qwen3-8B)")
        print(f"{'='*60}")
        
        # ä½¿ç”¨å·²çŸ¥çš„ Qwen3-8B åŸºçº¿åˆ†æ•°ï¼ˆå®Œæ•´è¯„ä¼°çš„ç»“æœï¼‰
        # è¿™äº›æ˜¯ä¹‹å‰ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¯„ä¼°å¾—åˆ°çš„ç»“æœ
        # æ¯”åœ¨è®­ç»ƒå‰è¿è¡Œ --limit è¯„ä¼°æ›´å‡†ç¡®
        known_baselines = {
            'mmlu_stem': 0.751,      # MMLU STEM: 75.1%
            'mmlu_other': 0.779,     # MMLU Other: 77.9%
            'gsm8k': 0.8802,         # GSM8K: 88.02%
        }
        
        for task in self.tasks:
            task_key = task.split(',')[0] if ',' in task else task
            if task_key in known_baselines:
                self.baseline_scores[task_key] = known_baselines[task_key]
                print(f"   {task_key}: {known_baselines[task_key]:.1%} (baseline)")
            else:
                print(f"   âš ï¸ {task_key}: No baseline available")
        
        # ä¿å­˜åˆå§‹çŠ¶æ€ï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰
        self._save_state(eval_output_dir)
        
        print(f"\nğŸ“ Note: Baselines from full evaluation of Qwen3-8B base model")
        print(f"   Training evaluations use --limit {self.limit} for speed")
        print(f"   Delta values show relative change from these baselines")
        print(f"{'='*60}\n")
    
    def _run_evaluation(
        self,
        model,
        args: TrainingArguments,
        state: TrainerState,
    ) -> Dict[str, float]:
        """
        è¿è¡Œ lm-eval è¯„ä¼°
        
        å¤ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­å·²ä¿å­˜çš„ checkpointï¼ˆç”± Trainer çš„ save_steps æ§åˆ¶ï¼‰
        lm_eval çš„ --limit å‚æ•°å›ºå®šå–å‰ N æ¡æ•°æ®ï¼Œä¿è¯æ¯æ¬¡è¯„ä¼°çš„æ ·æœ¬ä¸€è‡´
        """
        
        results = {}
        
        # è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
        base_model_path = getattr(model.config, '_name_or_path', None)
        if not base_model_path:
            base_model_path = os.environ.get('BASE_MODEL_PATH', '/mnt/input/models/Qwen3-8B')
        
        eval_output_dir = self.output_dir or args.output_dir
        
        # æŸ¥æ‰¾è®­ç»ƒä¿å­˜çš„æœ€æ–° checkpoint
        # Trainer æŒ‰ save_steps ä¿å­˜ checkpoint-{step} ç›®å½•
        checkpoint_dir = self._find_latest_checkpoint(eval_output_dir, state.global_step)
        
        if checkpoint_dir is None:
            print(f"âš ï¸ No checkpoint found at step {state.global_step}, skipping evaluation")
            print(f"   (Checkpoints are saved every {args.save_steps} steps)")
            return results
        
        try:
            # æ£€æµ‹ checkpoint ç±»å‹å¹¶æ„å»ºæ­£ç¡®çš„ model_args
            # FSDP å’Œæ ‡å‡† LoRA ä¿å­˜æ ¼å¼ä¸åŒ
            model_args = self._build_model_args(base_model_path, checkpoint_dir)
            
            eval_result_dir = os.path.join(eval_output_dir, f"_eval_results_step_{state.global_step}")
            os.makedirs(eval_result_dir, exist_ok=True)
            
            # ä½¿ç”¨å• GPU è¯„ä¼°ï¼Œé¿å…ä¸ FSDP è®­ç»ƒè¿›ç¨‹å†²çª
            # FSDP è®­ç»ƒå·²ç»å ç”¨äº†æ‰€æœ‰ GPU çš„åˆ†å¸ƒå¼ç¯å¢ƒ
            # åœ¨å›è°ƒä¸­å¯åŠ¨å¦ä¸€ä¸ª multi-GPU è¿›ç¨‹ä¼šå¯¼è‡´å†²çª
            # å• GPU + device_map=auto å·²ç»è¶³å¤Ÿå¿«ï¼ˆ100 samplesï¼‰
            cmd = [
                "lm_eval",
                "--model", "hf",
                "--model_args", model_args,
                "--tasks", ",".join(self.tasks),
                "--num_fewshot", str(self.num_fewshot),
                "--batch_size", str(self.batch_size),
                "--limit", str(self.limit),
                "--output_path", eval_result_dir,
            ]
            print(f"   ğŸ”„ Single-process evaluation (avoiding FSDP conflict)")
            
            print(f"   Checkpoint: {os.path.basename(checkpoint_dir)}")
            print(f"   Tasks: {', '.join(self.tasks)}")
            print(f"   Limit: {self.limit} samples per task (fixed, first N samples)")
            print(f"   Few-shot: {self.num_fewshot}")
            print(f"   Command: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=1800,  # 30 åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode != 0:
                print(f"âš ï¸ lm_eval failed with return code {result.returncode}")
                print(f"   STDOUT (last 2000 chars):")
                print(result.stdout[-2000:] if result.stdout else "(empty)")
                print(f"   STDERR (last 2000 chars):")
                print(result.stderr[-2000:] if result.stderr else "(empty)")
                return results
            
            results = self._parse_results(eval_result_dir)
            
        except subprocess.TimeoutExpired:
            print("âš ï¸ LM-Eval timed out after 30 minutes")
        except Exception as e:
            print(f"âš ï¸ LM-Eval error: {e}")
        
        return results
    
    def _build_model_args(self, base_model_path: str, checkpoint_dir: str) -> str:
        """
        æ ¹æ® checkpoint ç±»å‹æ„å»º lm_eval çš„ model_args
        
        æ”¯æŒçš„ checkpoint ç±»å‹ï¼š
        1. æ ‡å‡† LoRA adapter (adapter_config.json): ä½¿ç”¨ peft å‚æ•°åŠ è½½
        2. FSDP ä¿å­˜çš„ LoRA (pytorch_model_fsdp_0): éœ€è¦åˆå¹¶ååŠ è½½
        3. å®Œæ•´æ¨¡å‹ checkpoint (model.safetensors): ç›´æ¥åŠ è½½
        """
        
        # æ£€æµ‹ checkpoint ç±»å‹
        has_adapter_config = os.path.exists(os.path.join(checkpoint_dir, "adapter_config.json"))
        has_fsdp_sharded = os.path.exists(os.path.join(checkpoint_dir, "pytorch_model_fsdp_0"))
        has_adapter_safetensors = os.path.exists(os.path.join(checkpoint_dir, "adapter_model.safetensors"))
        has_adapter_bin = os.path.exists(os.path.join(checkpoint_dir, "adapter_model.bin"))
        
        print(f"   ğŸ“ Checkpoint type detection:")
        print(f"      adapter_config.json: {has_adapter_config}")
        print(f"      adapter_model.safetensors: {has_adapter_safetensors}")
        print(f"      adapter_model.bin: {has_adapter_bin}")
        print(f"      pytorch_model_fsdp_0 (FSDP): {has_fsdp_sharded}")
        
        # 1. æ ‡å‡† PEFT LoRA adapterï¼ˆæœ€å¸¸è§çš„æƒ…å†µï¼‰
        if has_adapter_config and (has_adapter_safetensors or has_adapter_bin):
            print(f"   âœ… Using standard PEFT adapter loading")
            return f"pretrained={base_model_path},peft={checkpoint_dir},trust_remote_code=True,device_map=auto"
        
        # 2. FSDP sharded checkpoint - è¿™ç§æƒ…å†µæ¯”è¾ƒå¤æ‚
        # FSDP with LoRA ä¼šä¿å­˜å®Œæ•´çš„æ¨¡å‹çŠ¶æ€ï¼ˆåŒ…æ‹¬ LoRA æƒé‡ï¼‰
        # lm_eval ä¸ç›´æ¥æ”¯æŒè¿™ç§æ ¼å¼ï¼Œéœ€è¦å…ˆæ‰‹åŠ¨åŠ è½½å¹¶ä¿å­˜ä¸ºæ ‡å‡†æ ¼å¼
        if has_fsdp_sharded:
            print(f"   âš ï¸ FSDP sharded checkpoint detected")
            print(f"   ğŸ“ FSDP checkpoints require manual loading and conversion")
            print(f"   Attempting to use base model + FSDP merged weights...")
            
            # å¯¹äº FSDPï¼Œlm_eval æ— æ³•ç›´æ¥åŠ è½½åˆ†ç‰‡ checkpoint
            # æ–¹æ¡ˆï¼šæ£€æŸ¥æ˜¯å¦æœ‰ FSDP ä¿å­˜çš„å®Œæ•´æ¨¡å‹
            # å¦‚æœæœ‰ model.safetensors æˆ– pytorch_model.binï¼Œå¯ä»¥ç›´æ¥ç”¨
            if os.path.exists(os.path.join(checkpoint_dir, "model.safetensors")):
                print(f"   âœ… Found merged model.safetensors, loading directly")
                return f"pretrained={checkpoint_dir},trust_remote_code=True,device_map=auto"
            elif os.path.exists(os.path.join(checkpoint_dir, "pytorch_model.bin")):
                print(f"   âœ… Found merged pytorch_model.bin, loading directly")
                return f"pretrained={checkpoint_dir},trust_remote_code=True,device_map=auto"
            else:
                # FSDP sharded æ ¼å¼ï¼Œlm_eval æ— æ³•ç›´æ¥å¤„ç†
                print(f"   âš ï¸ FSDP sharded format not directly supported by lm_eval")
                print(f"   Falling back to base model evaluation (no LoRA)")
                return f"pretrained={base_model_path},trust_remote_code=True,device_map=auto"
        
        # 3. å®Œæ•´æ¨¡å‹ checkpointï¼ˆå·²åˆå¹¶çš„æ¨¡å‹ï¼‰
        if os.path.exists(os.path.join(checkpoint_dir, "model.safetensors")):
            print(f"   âœ… Using full model checkpoint")
            return f"pretrained={checkpoint_dir},trust_remote_code=True,device_map=auto"
        
        # 4. Fallback: åªæœ‰ adapter_config.json ä½†æ²¡æœ‰æƒé‡æ–‡ä»¶ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
        if has_adapter_config:
            print(f"   âš ï¸ adapter_config.json found but no weights, trying anyway...")
            return f"pretrained={base_model_path},peft={checkpoint_dir},trust_remote_code=True,device_map=auto"
        
        # 5. æ— æ³•è¯†åˆ«çš„æ ¼å¼
        print(f"   âš ï¸ Unknown checkpoint format, using base model only")
        return f"pretrained={base_model_path},trust_remote_code=True,device_map=auto"

    def _find_latest_checkpoint(
        self,
        output_dir: str,
        current_step: int,
    ) -> Optional[str]:
        """
        æŸ¥æ‰¾æœ€è¿‘çš„å·²ä¿å­˜ checkpoint
        
        Trainer ä¿å­˜çš„ checkpoint æ ¼å¼ï¼šcheckpoint-{step}
        è¿”å›ä¸å½“å‰æ­¥æ•°æœ€æ¥è¿‘çš„ checkpoint è·¯å¾„
        """
        checkpoints = []
        
        if not os.path.exists(output_dir):
            return None
        
        for name in os.listdir(output_dir):
            if name.startswith("checkpoint-"):
                try:
                    step = int(name.split("-")[1])
                    checkpoint_path = os.path.join(output_dir, name)
                    # æ”¯æŒå¤šç§ checkpoint æ ¼å¼ï¼š
                    # 1. æ ‡å‡† LoRA: adapter_config.json
                    # 2. FSDP sharded: pytorch_model_fsdp_0 ç›®å½•
                    # 3. FSDP full state: pytorch_model.bin æˆ– model.safetensors
                    # 4. ä»»ä½• trainer_state.jsonï¼ˆTrainer æ€»æ˜¯ä¿å­˜è¿™ä¸ªï¼‰
                    checkpoint_markers = [
                        "adapter_config.json",          # Standard LoRA
                        "adapter_model.safetensors",    # LoRA safetensors
                        "pytorch_model_fsdp_0",         # FSDP sharded
                        "model.safetensors",            # Full model
                        "pytorch_model.bin",            # PyTorch format
                        "trainer_state.json",           # Trainer always saves this
                    ]
                    
                    is_valid = any(
                        os.path.exists(os.path.join(checkpoint_path, marker))
                        for marker in checkpoint_markers
                    )
                    
                    if is_valid:
                        checkpoints.append((step, checkpoint_path))
                except (ValueError, IndexError):
                    continue
        
        if not checkpoints:
            return None
        
        # æ‰¾åˆ°ä¸è¶…è¿‡å½“å‰æ­¥æ•°çš„æœ€è¿‘ checkpoint
        valid_checkpoints = [(s, p) for s, p in checkpoints if s <= current_step]
        if valid_checkpoints:
            return max(valid_checkpoints, key=lambda x: x[0])[1]
        
        # å¦‚æœæ²¡æœ‰ï¼Œè¿”å›æœ€æ—©çš„ï¼ˆç”¨äº baselineï¼‰
        return min(checkpoints, key=lambda x: x[0])[1]
    
    def _parse_results(self, result_dir: str) -> Dict[str, float]:
        """è§£æ lm_eval è¾“å‡ºçš„ç»“æœæ–‡ä»¶"""
        results = {}
        
        # æŸ¥æ‰¾ results.json æ–‡ä»¶
        for pattern in ["results.json", "**/results*.json"]:
            matches = list(Path(result_dir).glob(pattern))
            if matches:
                try:
                    with open(matches[0]) as f:
                        data = json.load(f)
                    
                    if "results" in data:
                        for task, task_data in data["results"].items():
                            # æå–ä¸»è¦æŒ‡æ ‡
                            score = None
                            for metric in [
                                "acc", "acc_norm", "exact_match",
                                "acc,none", "acc_norm,none",
                                "exact_match,flexible-extract",
                                "exact_match,strict-match",
                            ]:
                                if metric in task_data:
                                    score = task_data[metric]
                                    break
                            
                            if score is not None:
                                # ç®€åŒ–ä»»åŠ¡å
                                task_name = task.split(",")[0] if "," in task else task
                                results[task_name] = score
                    
                    break
                except Exception as e:
                    print(f"âš ï¸ Failed to parse results: {e}")
        
        return results
    
    def _log_results(
        self,
        results: Dict[str, float],
        step: int,
        args: TrainingArguments,
        is_baseline: bool = False,
    ):
        """è®°å½•è¯„ä¼°ç»“æœåˆ° WandB å’Œæœ¬åœ°"""
        
        # ä¿å­˜åˆ°å†å²
        record = {
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "is_baseline": is_baseline,
            "results": results,
        }
        
        # æ·»åŠ ä¸åŸºçº¿çš„å¯¹æ¯”
        if not is_baseline and self.baseline_scores:
            deltas = {}
            for task, score in results.items():
                if task in self.baseline_scores:
                    delta = score - self.baseline_scores[task]
                    deltas[f"{task}_delta"] = delta
            record["deltas"] = deltas
        
        self.eval_history.append(record)
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        eval_output_dir = self.output_dir or args.output_dir
        history_file = os.path.join(eval_output_dir, "lm_eval_history.json")
        try:
            with open(history_file, 'w') as f:
                json.dump(self.eval_history, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ Failed to save eval history: {e}")
        
        # ä¿å­˜ callback çŠ¶æ€ï¼ˆæ”¯æŒæ–­ç‚¹æ¢å¤ï¼‰
        self._save_state(eval_output_dir)
        
        # è®°å½•åˆ° WandB
        if self.log_to_wandb:
            try:
                import wandb
                if wandb.run is not None:
                    log_dict = {}
                    
                    # å§‹ç»ˆè®°å½•ç»å¯¹å€¼ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
                    for task, score in results.items():
                        log_dict[f"lm_eval/{task}"] = score
                    
                    # å¦‚æœæœ‰åŸºçº¿ï¼Œä¹Ÿè®°å½• deltaï¼ˆè¾…åŠ©æŒ‡æ ‡ï¼‰
                    if not is_baseline and self.baseline_scores:
                        for task, score in results.items():
                            if task in self.baseline_scores:
                                delta = score - self.baseline_scores[task]
                                log_dict[f"lm_eval/{task}_delta"] = delta
                        
                        # æ·»åŠ ç»¼åˆ delta æŒ‡æ ‡ï¼ˆæ‰€æœ‰ä»»åŠ¡çš„å¹³å‡å˜åŒ–ï¼‰
                        deltas = [
                            score - self.baseline_scores[task]
                            for task, score in results.items()
                            if task in self.baseline_scores
                        ]
                        if deltas:
                            log_dict["lm_eval/avg_delta"] = sum(deltas) / len(deltas)
                    
                    wandb.log(log_dict, step=step)
            except Exception as e:
                print(f"âš ï¸ Failed to log to WandB: {e}")
    
    def _print_summary(
        self,
        results: Dict[str, float],
        step: int,
        is_baseline: bool = False,
    ):
        """æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦"""
        
        print(f"\nğŸ“Š LM-Eval Results (Step {step}):")
        print("-" * 50)
        
        if is_baseline:
            print(f"{'Task':<25} {'Score':>10}")
            print("-" * 50)
            for task, score in results.items():
                print(f"{task:<25} {score:>9.1%}")
        else:
            print(f"{'Task':<25} {'Score':>10} {'Change':>10}")
            print("-" * 50)
            
            has_forgetting = False
            for task, score in results.items():
                if task in self.baseline_scores:
                    delta = score - self.baseline_scores[task]
                    delta_str = f"{delta:+.1%}"
                    
                    # æ£€æµ‹é—å¿˜
                    if delta < -0.05:  # è¶…è¿‡ 5% ä¸‹é™
                        status = "âš ï¸"
                        has_forgetting = True
                    elif delta < -0.02:  # 2-5% ä¸‹é™
                        status = "ğŸ“‰"
                    elif delta > 0.02:
                        status = "ğŸ“ˆ"
                    else:
                        status = "âœ…"
                    
                    print(f"{task:<25} {score:>9.1%} {delta_str:>8} {status}")
                else:
                    print(f"{task:<25} {score:>9.1%} {'N/A':>10}")
            
            if has_forgetting:
                print("\nâš ï¸ WARNING: Significant capability degradation detected!")
                print("   Consider: reducing learning rate, using more regularization,")
                print("   or mixing in general capability data.")
        
        print("-" * 50)


def create_lm_eval_callback(
    eval_steps: int = 100,  # Must be multiple of save_steps
    tasks: List[str] = None,
    limit: int = 100,
    enabled: bool = True,
) -> Optional[LMEvalCallback]:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»º LM-Eval Callback
    
    Args:
        eval_steps: è¯„ä¼°é—´éš”æ­¥æ•°
        tasks: è¯„ä¼°ä»»åŠ¡
        limit: æ¯ä¸ªä»»åŠ¡çš„æ ·æœ¬é™åˆ¶
        enabled: æ˜¯å¦å¯ç”¨
    
    Returns:
        LMEvalCallback å®ä¾‹æˆ– None
    """
    if not enabled:
        return None
    
    # æ£€æŸ¥ lm_eval æ˜¯å¦å¯ç”¨
    try:
        result = subprocess.run(
            ["lm_eval", "--help"],
            capture_output=True,
            timeout=10,
        )
        if result.returncode != 0:
            print("âš ï¸ lm_eval not available, disabling LM-Eval callback")
            return None
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print("âš ï¸ lm_eval not found, disabling LM-Eval callback")
        return None
    
    return LMEvalCallback(
        eval_steps=eval_steps,
        tasks=tasks or ['mmlu_stem', 'mmlu_other', 'gsm8k'],
        limit=limit,
    )
